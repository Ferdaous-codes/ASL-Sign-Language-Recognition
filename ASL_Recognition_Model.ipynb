
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import os\n",
    "\n",
    "# --- 1. T√âL√âCHARGEMENT DES DONN√âES ---\n",
    "print(\"‚è≥ T√©l√©chargement des fichiers CSV...\")\n",
    "!wget -q --show-progress https://storage.googleapis.com/learning-datasets/sign_mnist_train.csv\n",
    "!wget -q --show-progress https://storage.googleapis.com/learning-datasets/sign_mnist_valid.csv\n",
    "\n",
    "# --- 2. CHARGEMENT ET PR√âPARATION (NOMM√â VALID) ---\n",
    "if os.path.exists(\"sign_mnist_train.csv\"):\n",
    "    print(\"‚úÖ Fichiers re√ßus. Pr√©paration des donn√©es...\")\n",
    "    train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "    valid_df = pd.read_csv(\"sign_mnist_test.csv\") # On charge le fichier test dans valid_df\n",
    "    \n",
    "    # S√©paration et Normalisation\n",
    "    x_train = train_df.drop('label', axis=1) / 255\n",
    "    x_valid = valid_df.drop('label', axis=1) / 255\n",
    "    \n",
    "    # Encodage des √©tiquettes (25 classes pour l'alphabet ASL)\n",
    "    y_train = tf.keras.utils.to_categorical(train_df['label'], 25)\n",
    "    y_valid = tf.keras.utils.to_categorical(valid_df['label'], 25)\n",
    "else:\n",
    "    print(\"‚ùå Erreur : Fichiers manquants.\")\n",
    "\n",
    "# --- 3. ARCHITECTURE DU MOD√àLE ---\n",
    "model = Sequential([\n",
    "    Dense(units=512, activation='relu', input_shape=(784,)),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dense(units=25, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- 4. ENTRA√éNEMENT AVEC VALIDATION ---\n",
    "print(\"\\nüöÄ Lancement de l'apprentissage...\")\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_data=(x_valid, y_valid) # Utilisation des donn√©es de validation\n",
    ")\n",
    "\n",
    "# --- 5. SAUVEGARDE ---\n",
    "model.save('asl_model.h5')\n",
    "print(\"\\n‚ú® TERMIN√â ! Mod√®le sauvegard√© sous 'asl_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fa1921-4a8e-47c6-bd8c-a2336c8974be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ T√©l√©chargement des fichiers CSV...\n",
      "‚úÖ Fichiers re√ßus. Pr√©paration des donn√©es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 16:23:45.614569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-05 16:23:47.414464: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Lancement de l'apprentissage...\n",
      "Epoch 1/20\n",
      "858/858 [==============================] - 5s 5ms/step - loss: 1.9388 - accuracy: 0.3840 - val_loss: 1.9462 - val_accuracy: 0.4083\n",
      "Epoch 2/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.9505 - accuracy: 0.6757 - val_loss: 1.5208 - val_accuracy: 0.5534\n",
      "Epoch 3/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.5735 - accuracy: 0.8056 - val_loss: 1.0573 - val_accuracy: 0.6792\n",
      "Epoch 4/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.3902 - accuracy: 0.8741 - val_loss: 1.1146 - val_accuracy: 0.7124\n",
      "Epoch 5/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.2837 - accuracy: 0.9116 - val_loss: 0.7082 - val_accuracy: 0.8164\n",
      "Epoch 6/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.2454 - accuracy: 0.9327 - val_loss: 1.0106 - val_accuracy: 0.7858\n",
      "Epoch 7/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.2067 - accuracy: 0.9457 - val_loss: 0.9341 - val_accuracy: 0.8108\n",
      "Epoch 8/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1879 - accuracy: 0.9529 - val_loss: 0.8947 - val_accuracy: 0.8256\n",
      "Epoch 9/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1832 - accuracy: 0.9572 - val_loss: 1.2772 - val_accuracy: 0.7757\n",
      "Epoch 10/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1626 - accuracy: 0.9642 - val_loss: 0.9812 - val_accuracy: 0.8470\n",
      "Epoch 11/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1552 - accuracy: 0.9687 - val_loss: 1.0390 - val_accuracy: 0.8698\n",
      "Epoch 12/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1589 - accuracy: 0.9672 - val_loss: 1.3190 - val_accuracy: 0.7818\n",
      "Epoch 13/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1269 - accuracy: 0.9720 - val_loss: 0.9750 - val_accuracy: 0.8631\n",
      "Epoch 14/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1300 - accuracy: 0.9739 - val_loss: 1.3717 - val_accuracy: 0.8097\n",
      "Epoch 15/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1183 - accuracy: 0.9780 - val_loss: 1.3998 - val_accuracy: 0.8042\n",
      "Epoch 16/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1362 - accuracy: 0.9751 - val_loss: 1.8527 - val_accuracy: 0.7968\n",
      "Epoch 17/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1204 - accuracy: 0.9771 - val_loss: 1.3059 - val_accuracy: 0.8377\n",
      "Epoch 18/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1027 - accuracy: 0.9807 - val_loss: 1.2561 - val_accuracy: 0.8709\n",
      "Epoch 19/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.1134 - accuracy: 0.9784 - val_loss: 1.3460 - val_accuracy: 0.8454\n",
      "Epoch 20/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.1017 - accuracy: 0.9819 - val_loss: 2.4075 - val_accuracy: 0.7366\n",
      "\n",
      "‚ú® TERMIN√â ! Mod√®le sauvegard√© sous 'asl_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import os\n",
    "\n",
    "# --- 1. T√âL√âCHARGEMENT DES DONN√âES ---\n",
    "print(\"‚è≥ T√©l√©chargement des fichiers CSV...\")\n",
    "!wget -q --show-progress https://storage.googleapis.com/learning-datasets/sign_mnist_train.csv\n",
    "!wget -q --show-progress https://storage.googleapis.com/learning-datasets/sign_mnist_valid.csv\n",
    "\n",
    "# --- 2. CHARGEMENT ET PR√âPARATION (NOMM√â VALID) ---\n",
    "if os.path.exists(\"sign_mnist_train.csv\"):\n",
    "    print(\"‚úÖ Fichiers re√ßus. Pr√©paration des donn√©es...\")\n",
    "    train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "    valid_df = pd.read_csv(\"sign_mnist_valid.csv\") # On charge le fichier test dans valid_df\n",
    "    \n",
    "    # S√©paration et Normalisation\n",
    "    x_train = train_df.drop('label', axis=1) / 255\n",
    "    x_valid = valid_df.drop('label', axis=1) / 255\n",
    "    \n",
    "    # Encodage des √©tiquettes (25 classes pour l'alphabet ASL)\n",
    "    y_train = tf.keras.utils.to_categorical(train_df['label'], 25)\n",
    "    y_valid = tf.keras.utils.to_categorical(valid_df['label'], 25)\n",
    "else:\n",
    "    print(\"‚ùå Erreur : Fichiers manquants.\")\n",
    "\n",
    "# --- 3. ARCHITECTURE DU MOD√àLE ---\n",
    "model = Sequential([\n",
    "    Dense(units=512, activation='relu', input_shape=(784,)),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dense(units=25, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- 4. ENTRA√éNEMENT AVEC VALIDATION ---\n",
    "print(\"\\nüöÄ Lancement de l'apprentissage...\")\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_data=(x_valid, y_valid) # Utilisation des donn√©es de validation\n",
    ")\n",
    "\n",
    "# --- 5. SAUVEGARDE ---\n",
    "model.save('asl_model.h5')\n",
    "print(\"\\n‚ú® TERMIN√â ! Mod√®le sauvegard√© sous 'asl_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab8681-090a-478c-a99a-d2c8f1eb7897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
